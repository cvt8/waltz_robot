\documentclass{amsart}
\usepackage[style=alphabetic,backend=biber,sorting=nty]{biblatex}
\addbibresource{biblio.bib}
\usepackage[T1]{fontenc}
\usepackage[french,english]{babel}
\usepackage[margin=0.6in]{geometry}
\usepackage{macros}
\usepackage{csquotes}
\usepackage{subfiles, caption, listings}
\usepackage[]{amsmath}
\usepackage{amssymb}
\usepackage{dsfont}
\usepackage{stmaryrd}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{varwidth}
\usepackage{float}
\usepackage{amsthm}
\usepackage{verbatim}
\usepackage{enumitem}
\usepackage[most]{tcolorbox}
\usepackage[ruled]{algorithm2e}
\usepackage{relsize}
\usepackage{mathtools}
\usepackage{csquotes}
\usepackage{wrapfig}
\usepackage[colorinlistoftodos]{todonotes}
\renewcommand\thesection{\arabic{section}}

\newcommand{\lref}[1]{\mbox{\thref{#1}}}

%THEOREMS
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\setlist[enumerate,1]{label={(\roman*)}}

\date{December 20, 2024}
\title{Waltz right turn with a Humanoid Robot using Inverse Kinematics}
\author{Constantin Vaillant-Tenzer, Charles Monté}

\begin{document}

\maketitle

\begin{abstract}
  We developed a system enabling a humanoid robot to perform the waltz right turn, 
  a foundational movement in ballroom dancing, using inverse kinematics. 
  The pipeline integrates music tempo synchronization and joint movement extraction derived from video data. 
  By combining a neural inverse kinematics framework with tailored pre-processing techniques, 
  the robot's movements were fine-tuned to mimic human precision and elegance. 
  Results demonstrated the robot's ability to perform synchronized and visually accurate dance movements, 
  showcasing potential applications in human-robot interaction and artistic robotics
\end{abstract}


\section{Introduction}

The waltz, celebrated for its graceful and seamless movements, is a traditional ballroom dance that emerged in late 18th-century Europe. Derived from the German word walzen, meaning "to turn" or "to glide," the waltz is characterized by its rotational patterns and its distinctive 3/4 time rhythm. As one of the most iconic and enduring dances in both social and competitive settings, it emphasizes close partner coordination and fluid motion. Among its foundational figures, the right-turn holds particular importance, as it involves a couple rotating around a shared axis in a counterclockwise direction, showcasing the waltz's signature elegance. Despite its prominence in dance culture, the waltz—and ballroom dancing in general—remains an underexplored domain in the field of robotic movement and human-robot interaction.

In robotics research, various methods have been developed to enable robots to learn, imitate, and respond to human dance movements. The learning from observation approach \cite{traditional_jap_dance} enables robots to analyze and replicate key leg movements by observing human dancers, while real-time gesture-responsive frameworks \cite{spectacle_imitation} empower robots to generate dynamic and unpredictable artistic responses to live human input. Specific to ballroom dancing, efforts such as the development of a dance partner robot \cite{ballroom_dance} have focused on achieving human-robot coordination by predicting a human partner's intended movements through physical interaction.

This work aims to bridge the gap between robotic motion control and the artistry of dance by programming a humanoid robot to perform the waltz's iconic right-turn. Using inverse kinematics as the foundation, we developed a pipeline for movement acquisition and execution. This involved capturing the temporal positions of key joints—feet, pelvis, hands, and head—using insights from existing research. These joint positions were then used to solve inverse kinematics equations, ensuring the robot followed the ideal waltz trajectory with precision. Furthermore, we synchronized the robot's movements to match the rhythm of a selected musical piece, adapting its tempo to align with the beats per minute (BPM) of the waltz. This integration of movement and rhythm brings us closer to the goal of robots executing sophisticated and synchronized dance sequences in partnership with humans or autonomously.




\section{Theory Behind the Movement}

The movement of a dancer during a right turn in the waltz can be effectively modeled by analyzing the trajectory of the dancer’s center of gravity (CoG). 

In the waltz, the dancer’s motion can be decomposed into two main components. First, the dancer moves along the perimeter of an ellipse, commonly referred to as the \textbf{ball circle}. Concurrently, the dancer performs a rotation around their own axis, tracing a circular path with a diameter of approximately 1 meter. 

Both of these movements are assumed to occur with constant angular velocities.

The dancer completes one full rotation around their own axis every 360 beats of the music. Given the known beats per minute (BPM) of the music, the angular velocity around the circle is given by:

$$\omega_C = \frac{-2\pi \, \text{BPM}}{6 \times 60}.$$

Simultaneously, the dancer moves along the perimeter of the ball circle, completing one revolution every minute. This results in an angular velocity around the ellipse of:

$$\omega_E = \frac{-2\pi}{60}.$$

These two combined motions define the trajectory of the dancer's center of gravity, which we aim to replicate in the robotic model. The simulated movement of the center of gravity over two minutes is illustrated in Figure~\ref{fig:cog_movement}.

\begin{figure}
  \centering
  \includegraphics[width = 0.5 \columnwidth]{img/waltz_cog_movement.png} 
  \caption{Simulated movement of a Waltz dancer's center of gravity over 2 minutes.}\label{fig:cog_movement}
\end{figure}


\section{Our method}

\subsection{Overview of the method}
To simulate the trajectory done by a dancer with a robot, we have trained a humanoid robot to follow the sequence of movements done by the dancer during a waltz right turn.

To do so, we decided to mainly focus on how well the joints would follow the movement, without centering ourselves on physical constraints like making sure that the robot's feet are always on the ground.
However, we make sur, by using Pink \cite{pink2024} that the center of mass remains stable and that the robot does not fall.

To acquire the kinematics of the waltz right turn with the help of a video and then solve inverse kinematics with a humanoid robot model to fit the acquired movement. 
After having a robot able to dance the waltz right turn, we only had to fit the robot's movement to the music BPM, by making sure a right turn had the right duration.

We did not have sufficient time to implement everything ourselves from kinematics acquisition to inverse kinematics solving so we decided to center ourselves around acquiring a good base for the movement kinematics. 

Thus, we mainly relied on already implemented blocks to fit inside our pipeline while trying to get the best approximation of a waltz right turn possible.

The global pipeline we followed for our implementation is represented in Figure~\ref{fig:pipeline}.

\begin{figure}
  \includegraphics[width = 0.75 \columnwidth]{img/final_solution_pipeline.png}
  \caption{Our approach's full pipeline}\label{fig:pipeline}
\end{figure}

\subsection{Kinematics Extraction}
Our first idea of Kinematics Extraction was to both extract joint positions and joint angles, to ensure a perfect movement for the robot throughout the waltz movement. This idea did not work in the end, which made us go back to only joint position extraction while putting no constraints on the robot's joint angles, to make the inverse kinematics process find the optimal joint angles itself. In this subsection, we are going to chronologically follow our thought process and implementations of kinematics extraction.

We first decided to focus on two methods, one was proposed by Stéphane Caron at the start of the project : Estimating 3D Motion and Forces of Person-Object Interactions From Monocular Video\cite{Li_2019}, and the second was discovered during research about state-of-the-art kinematics extraction algorithms: NIKI, Neural Inverse Kinematics with Invertible Neural Networks for 3D Human Pose and Shape Estimation\cite{li2023niki}.

We did not focus long on the first method as its implementation was made using outdated projects (HMR for instance) which made the global implementation too tiresome. NIKI on the other hand was perfect for us, as it extracted both the joint positions and the joint angles at the same time. While the joint positions were easy to understand and use (with some tinkering to find each Joint Id - see NIKI results exploitation notebook in the GitHub), the joint angles did not follow a shape we understood. We were provided with fewer angles than the number of joints and no idea of which joint they were related to. We did not manage to understand how to extract each joint's angle over time, even after carefully reading the related paper and source code. 

\begin{figure}
  \includegraphics[width = 0.99 \columnwidth]{img/pose2sim_pipeline.png}
  \caption{The Pose2Sim pipeline : This solution extracts 2D keypoints coordinates (using RTM Pose\cite{RTMPose}) to produce an OpenSim result (full-body 3D joint angles), which we use in our pipeline by extracting the information that is interesting from us, ie. the RTMPose output and the full-body 3D joint angles.}
  \label{fig:pose2sim_pipeline}
\end{figure}

\begin{figure}
  \includegraphics[width = 0.45 \columnwidth]{img/image-000001.jpg}
  \includegraphics[width = 0.45 \columnwidth]{img/image-000130.jpg}
  \caption{Example of the video we used for the kinematics extraction with niki \cite{li2023niki}. Left: First frame of the video, Right: Middle frame of the video}
  \label{fig:nikiimage}
\end{figure}

Thus, we decided to find a new pipeline that could extract both, in an intelligible manner: Pose2Sim\cite{Pose2Sim}. Pose2Sim has been the main pipeline we have used for the extraction of joint positions and angles, mainly because the outputs were directly linking each joint name to an angle and a position. Pose2Sim's global pipeline can be found in Figure~\ref{fig:pose2sim_pipeline}. This solution is not that different from the two proposed above, they follow a similar chain of thought which consists of a 2D pose estimation algorithm (RTM Pose\cite{RTMPose} for Pose2Sim), followed by a block that projects those coordinates to the 3D space, before doing inverse kinematics to find the optimal joint angles to adapt the joint positions to a humanoid model. 

Even if only using the first part of all those implementations to extract the 3D joint positions before using our own Inverse Kinematics to get the optimal joint angles now seems obvious, we lost a lot of time trying to use both the joint positions and angles to try and get a perfect movement. We first implemented a global pipeline using the Pose2Sim results, which we presented during the poster session. It did not work due to bad adaptations of the joints to the angles and the positions, which blocked the robot in place, only allowing its feet to move. 

After reflecting on our implementation, we managed to make the robot move but noticed that both the Pose2Sim and NIKI extracted joint positions were not parallel to the $(x, y)$ plane. Thus, we decided to find optimal transformation matrices (composition of rotations, translations, and scaling for the $x$, $y$, and $z$-axis) but did not manage to find one, manually or with optimization algorithms, to make Pose2Sim results exploitable. The reasons we identified for it were two-fold.

\begin{enumerate}
    \item The transformation applied to the movement was too difficult for us to estimate manually and we did not manage to find optimal cost algorithms to have a good transformation matrix output.
    \item The Pose2Sim pipeline adapts the joint positions to the 3D space with the use of the extrinsic and intrinsic camera parameters while assuming that the camera is static. However, in the video we used, the camera moves during the acquisition which throws off the Pose2Sim estimation causing a huge change in $x,y,z$ coordinated in the middle of the movement, as seen in Figure~\ref{fig:x_coord_pose2sim}.
\end{enumerate}
\begin{figure}
  \includegraphics[width = 0.33 \columnwidth]{img/x_coord_pose2sim.png}
  \caption{x-coordinate of each joints over time with Pose2Sim extraction}\label{fig:x_coord_pose2sim}
\end{figure}
\begin{figure}
  \includegraphics[width = 0.5 \columnwidth]{img/influence_of_smoothing.png}
  \caption{Influence of smoothing on the coordinates of the pelvis joint over time}\label{fig:influence_of_smoothing}
\end{figure}
\begin{figure}
  \includegraphics[width = 0.75 \columnwidth]{img/influence_transformation.png}
  \caption{Influence of the transformation stage. Left: Before transformation, Right: After transformation}\label{fig:influence_transformation}
\end{figure}

Looking at all the extraction we had made, we concluded that NIKI's way of extracting joint positions, based on a CNN backbone output robust to camera movements which is then enhanced by the NIKI solver (which is an Invertible Neural Network for Inverse Kinematics solving), was the best solution for us to extract the joint positions. Only using the joint positions extracted thanks to the NIKI algorithm, pre-processing them using smoothing functions for each coordinate and making them parallel to the $(x, y)$ plane worked for us. The main reason why NIKI performed better for us was because the transformation matrix was easier to find and the joint positions were more stable over time.

The smoothing part was important as we noticed a lot of noise on the prediction of the center of mass' position, especially in the $z$-axis. However, this position was directly used as the position of the pelvis joint and added to each joint position to make the robot move in the 3D space and avoid it being anchored at the origin. The use of smoothing was crucial for our implementation, as the noise was highly impacting the quality of the feet's movement. To reduce it, we used a savgol filter to smooth the coordinates, its influence can be seen in Figure~\ref{fig:influence_of_smoothing}.

The transformation of the coordinates to make them parallel to the $(x, y)$ plane discussed earlier has been done by finding the optimal rotation matrix that would make the pelvis joint's $z$-coordinate equal to half the robot's height. We did not manage to find a way to do it automatically so we had to find the optimal rotation matrix manually. The influence of the transformation can be seen in Figure~\ref{fig:influence_transformation}.

We then only had to solve the inverse kinematics using those joint positions to find the optimal angles automatically.

\subsection{Choice of the Robot}

We examined various humanoid robot models through the Robot Descriptions in Python repository \cite{robot_descriptions_py} to identify one with joint configurations most similar to the output of our Pose2Sim model. After evaluating several options, we selected AtlasV4 as our humanoid model due to its proven ability to perform complex and dynamic movements. This choice was driven by AtlasV4's advanced capabilities, which align well with the type of engaging, fluid motion required for our project. However, it is important to note that our code is designed with flexibility in mind and can be easily adapted to other humanoid robots. To do so, only the joint names would need to be modified accordingly.


\subsection{Inverse Kinematics}
To solve the inverse kinematics, we applied Pink\cite{pink2024} which solves differential inverse kinematics by weighted tasks. 

The method uses residual functions of the robot configuration $q$ that should be driven to zero to make the robot do a certain task. In our case, we for instance try to put the robot's feet at a certain position $p_{\text{feet}}^*$ so an example of a residual would be $e(q) = p_{\text{feet}}^* - p_{\text{feet}}(q)$. 

To solve the equation system produced, the method computes a velocity $v$ that satisfies the equation $J_e(q)v = \dot{e}(q) = -\alpha e(q)$ - $J_e(q)$ being the Jacobian of task $e$ - for each residual. 
It is of course not possible so the method finds the optimal solution to the following minimization problem, which finds the movement that best solves all of the tasks at the same time.
$$
\begin{aligned}
\min_v \ \ &\sum_{\text{tasks} \ e} ||J_e(q)v + \alpha e(q)||^2 \\
\text{subject to} \ \ \ &v_{min}(q) \leq v \leq v_{max}(q)
\end{aligned}
$$

Regarding the python side of things, as Pink revolves around the introduction of tasks, we had to choose which tasks we wanted to solve. 
We decided to focus on the feet, pelvis, hands, and head, as they are the most important parts of the body for a waltz right turn. 

We forced all those joints to follow the positions extracted from the NIKI pipeline using FrameTasks, which are tasks that force a certain frame to follow a certain position and orientation. To keep the robot steady, we added a PostureTask that forces the robot to stay in a certain posture, which we chose to be the initial posture of the robot.

Concerning the weights, we decided to put all the weights on the joint orientations to 0, and only play on the weights of the joint positions. We decided to put the weights of the feet, the pelvis and the head higher than the weights of the hands, as the hands are not as important as the other parts of the body for a waltz right turn, but are necessary to ensure a rotation of the robot's body.

\subsection{Rhythm adaptation}

To adapt the rhythm of the movement to the BPM of the music, we simply decided to calculate the ideal time between each frame of the movement to adapt it to the BPM of the music. 

To do so, we calculated the time it would take the robot to do 4 full turns around the small circle at the new BPM (as Constantin does 4 full turns during the video) 
and divide the number of frames by this time to get the new ideal frame rate. 
The new ideal time between each frame is then the inverse of this frame rate.

\section{Our Results}

In this section, we present the results of our approach, demonstrating the robot's ability to execute precise movements and synchronize with the music. 
To illustrate this, we first show images depicting the motion of the pelvis and the right foot during a waltz turn. These images highlight the robot's capacity to perform the necessary movements accurately, 
following the rhythm of the music, even at fast tempos (e.g., 187 BPM). 
The high frame rate in our simulation allows for a detailed breakdown of the dance movements, ensuring fluidity and precision in execution.

We have generated a video showcasing these results, which is available on \href{https://www.youtube.com/watch?v=1MIFP3BURI0}{YouTube}. 
This video demonstrates the robot’s dancing capabilities, including its ability to stay in sync with the music’s rhythm. 
The dance movements can be recreated with any music of your choice by following the instructions provided in the code on GitHub. 
Additionally, we provide a waltz movement file that can be adapted to different room configurations, offering flexibility for various performance scenarios. 
This file allows for customizability in the dance, including adjustments to suit different physical spaces.

\begin{figure}[h!]
  \centering
  \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/Pelvis_positions_with_color_scale.png}
    \caption{Illustration of the pelvis movement during a waltz turn. The robot accurately executes the required motions.}
    \label{fig:pelvis_turn}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\textwidth}
    \centering
    \includegraphics[width=\textwidth]{img/Right Foot_positions_with_color_scale}
    \caption{Illustration of the right foot movement during a waltz turn. The robot's precise footwork follows the rhythm of the music.}
    \label{fig:right_foot_turn}
  \end{minipage}
\end{figure}

For added artistic effect, we incorporated a visual element where the robot appears to float and eventually disappear, creating a dreamlike atmosphere. 
While this effect adds to the visual appeal, it should be noted that in the actual performance, the robot remains grounded. 
The disappearing effect is achieved through camera settings, which could be further refined in future iterations of the code.

The entire process, from movement generation to synchronization with music, is highly customizable. By entering execution commands (as detailed in the GitHub repository), users can adjust various parameters to suit their specific needs. 
This flexibility makes the approach adaptable to different environments, styles of music, and movement configurations.

\begin{figure}
  \includegraphics[width = 0.7 \columnwidth]{img/ball.png}
  \caption{Our robot dancing in a ballroom}\label{fig:ballroom}
\end{figure}


\section{Discussion}

While the results obtained in this study are promising, several avenues for further development have been identified that could enhance the performance and applicability of the proposed approach. 
The following points outline key areas for improvement:

\begin{enumerate} 
  \item \textbf{Incorporation of a control module for constant angular velocity:} One potential enhancement is the integration of a control module to maintain a constant angular velocity during the small circle of the waltz turn. 
  Achieving this precise control, which is challenging for human dancers, would contribute to even smoother and more consistent motion. 
  By stabilizing the angular velocity, the robot’s movement could be rendered with greater accuracy and elegance, further enhancing the visual appeal of the performance.
  \item \textbf{Stabilization of balance and ground contact:} A critical consideration is the maintenance of the robot’s balance throughout the dance, particularly during rotational movements. 
  Ensuring that the robot remains stable and maintains proper ground contact during the execution of the waltz turn is essential for realistic performance. 
  This could be achieved through the integration of balance control algorithms that dynamically adjust the robot's posture and movement to compensate for any deviations in real-time.

  \item \textbf{Partnered dancing using reinforcement learning:} An exciting direction for future work is enabling the robot to dance with a human or another robot. 
  This would require the application of reinforcement learning (RL) techniques to allow the robot to dynamically adjust to the movements of its partner. 
  RL could be used to train the robot to learn the necessary coordination, synchronization, and interaction required to perform the waltz in a partnered setting, adding an additional layer of complexity and realism to the dance.
  
  \item \textbf{Real-world testing and adaptation:} While the simulation results are promising, it is imperative to test the system on a real robot to validate the performance in practical scenarios. 
  In particular, the robot will need to adapt to physical constraints such as friction, ground surface variability, and its own weight distribution. 
  These real-world factors may influence the precision and smoothness of the dance movements, and the control system will need to be calibrated to address these challenges effectively.
\end{enumerate}

  Addressing these challenges will not only improve the fidelity of the robot’s movements but also expand the potential applications of this work. 
  Future efforts should focus on the refinement of control algorithms, ensuring robust performance in dynamic environments, and exploring human-robot interaction in a collaborative dance context. 
  By overcoming these limitations, it is possible to achieve even more realistic, interactive, and engaging robotic dance performances.

  \section{Conclusion}

  This work presents a novel approach to programming humanoid robots for artistic movement, exemplified by the waltz right turn. 
  By leveraging advanced kinematics extraction methods, a humanoid model, 
  and synchronization with musical tempo, we achieved a fluid and precise representation of this classical dance. 
  The resulting pipeline is highly adaptable, supporting customization for different humanoid platforms, music styles, and movement configurations.

  Despite these promising results, several challenges remain. 
  These include refining the robot's balance, ensuring consistent angular velocities, and addressing physical constraints such as maintaining foot contact with the ground. 
  Future work could explore integrating advanced control modules, expanding the repertoire of dance movements, and enhancing real-time human-robot interactions in co-creative settings. 
  Ultimately, this research bridges the gap between robotics and performing arts, opening new pathways for robots to contribute to creative and cultural expressions.
  

\subsection*{Contributions:} Charles Monté and Constantin Vaillant-Tenzer contributed equally to this work. Charles Monte focused on the kinematics extraction part, while Constantin focused on the video generation, rythm synchrony, robot adaptation and benchmark. 
Both Constantin Vaillant Tenzer and Charles Monte worked on the inverse kinematics part.

\subsection*{Supplementary Material.} You can access the full code and further instructions on our \href{https://github.com/cvt8/waltz_robot}{GitHub page}.
You can also watch the video of the robot dancing the waltz right turn on \href{https://www.youtube.com/watch?v=1MIFP3BURI0}{YouTube}.

\subsection*{Acknowledgements.} We would like to thank Stépane Scaron for his guidance and support throughout this project. 
We thank all the teaching community of the robotic class for giving us the opportunity to work on such an interesting project.

\printbibliography[]

\end{document}
